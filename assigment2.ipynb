{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef5aaa-25f7-4582-8437-b86971cd8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fc_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a fully-connected layer.\n",
    "\n",
    "    The input x has shape (N, Din) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (Din,).\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array of shape (N, Din) giving input data\n",
    "    - w: A numpy array of shape (Din, Dout) giving weights\n",
    "    - b: A numpy array of shape (Dout,) giving biases\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, Dout)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass. Store the result in out.              #\n",
    "    ###########################################################################\n",
    "\n",
    "    # ÎŸ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ out=ð‘¥â‹…ð‘¤+ð‘ ,\n",
    "    # out=xâ‹…w+b Ï€ÏÎ¿ÎºÏÏ€Ï„ÎµÎ¹ Î±Ï€ÏŒ Ï„Î¿Î½ Ï„ÏÏŒÏ€Î¿ Î¼Îµ Ï„Î¿Î½ Î¿Ï€Î¿Î¯Î¿ Î»ÎµÎ¹Ï„Î¿Ï…ÏÎ³ÎµÎ¯ Î­Î½Î± Ï€Î»Î®ÏÏ‰Ï‚ ÏƒÏ…Î½Î´ÎµÎ´ÎµÎ¼Î­Î½Î¿ ÎµÏ€Î¯Ï€ÎµÎ´Î¿ (fully connected layer) ÏƒÎµ Î­Î½Î± Î½ÎµÏ…ÏÏ‰Î½Î¹ÎºÏŒ Î´Î¯ÎºÏ„Ï…Î¿.\n",
    "    out = np.dot(x, w) + b\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_backward(grad_out, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a fully-connected layer.\n",
    "\n",
    "    Inputs:\n",
    "    - grad_out: Numpy array of shape (N, Dout) giving upstream gradients\n",
    "    - cache: Tuple of:\n",
    "      - x: A numpy array of shape (N, Din) giving input data\n",
    "      - w: A numpy array of shape (Din, Dout) giving weights\n",
    "      - b: A numpy array of shape (Dout,) giving biases\n",
    "\n",
    "    Returns a tuple of downstream gradients:\n",
    "    - grad_x: A numpy array of shape (N, Din) of gradient with respect to x\n",
    "    - grad_w: A numpy array of shape (Din, Dout) of gradient with respect to w\n",
    "    - grad_b: A numpy array of shape (Dout,) of gradient with respect to b\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    grad_x, grad_w, grad_b = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for the fully-connected layer         #\n",
    "    ###########################################################################\n",
    "   \n",
    "    # ÎŸ Ï„ÏÏ€Î¿Ï‚ Î³Î¹Î± Ï„Î·Î½ Ï€Î±ÏÎ¬Î³Ï‰Î³Î¿ Ï‰Ï‚ Ï€ÏÎ¿Ï‚ x ÎµÎ¯Î½Î±Î¹: dLoss/dx = dLoss/dout * dout/dx = dLoss/dout * w^T\n",
    "    grad_x = np.dot(grad_out, w.T)\n",
    "    \n",
    "   \n",
    "    # ÎŸ Ï„ÏÏ€Î¿Ï‚ Î³Î¹Î± Ï„Î·Î½ Ï€Î±ÏÎ¬Î³Ï‰Î³Î¿ Ï‰Ï‚ Ï€ÏÎ¿Ï‚ w ÎµÎ¯Î½Î±Î¹: dLoss/dw = dLoss/dout * dout/dw = x^T * dLoss/dout\n",
    "    grad_w = np.dot(x.T, grad_out)  \n",
    "    \n",
    "   \n",
    "    # ÎŸ Ï„ÏÏ€Î¿Ï‚ Î³Î¹Î± Ï„Î·Î½ Ï€Î±ÏÎ¬Î³Ï‰Î³Î¿ Ï‰Ï‚ Ï€ÏÎ¿Ï‚ b ÎµÎ¯Î½Î±Î¹: dLoss/db = Î£ ( dLoss/dout_i * dout_i/db )\n",
    "    grad_b = np.sum(grad_out, axis=0)  # dLoss/db = Î¬Î¸ÏÎ¿Î¹ÏƒÎ¼Î±(dLoss/dout)\n",
    "    \n",
    "  \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return grad_x, grad_w, grad_b\n",
    "\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for the Rectified Linear Unit (ReLU) nonlinearity\n",
    "\n",
    "    Input:\n",
    "    - x: A numpy array of inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: A numpy array of outputs, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    out = np.zeros_like(x)\n",
    "\n",
    "    # Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ ReLU ÏƒÎµ ÎºÎ¬Î¸Îµ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î¿ Ï„Î¿Ï… x\n",
    "    for i in range(x.shape[0]):\n",
    "      for j in range(x.shape[1]):\n",
    "          out[i, j] = max(0, x[i, j])\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(grad_out, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a Rectified Linear Unit (ReLU) nonlinearity\n",
    "\n",
    "    Input:\n",
    "    - grad_out: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - grad_x: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    grad_x, x = None, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    grad_x = np.zeros_like(x)  # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎµÎ½ÏŒÏ‚ Ï€Î¯Î½Î±ÎºÎ± Î¼Î·Î´ÎµÎ½Î¹ÎºÏŽÎ½ Î¼Îµ Ï„Î¿ Î¯Î´Î¹Î¿ ÏƒÏ‡Î®Î¼Î± Î¼Îµ Ï„Î¿ x\n",
    "\n",
    "    # Î•Ï†Î±ÏÎ¼Î¿Î³Î® Ï„Î·Ï‚ ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·Ï‚ ReLU ÏƒÎµ ÎºÎ¬Î¸Îµ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Î¿ Ï„Î¿Ï… x\n",
    "    for i in range(x.shape[0]):\n",
    "      for j in range(x.shape[1]):\n",
    "        if x[i, j] > 0:\n",
    "            grad_x[i, j] = grad_out[i, j]\n",
    "        else:\n",
    "            grad_x[i, j] = 0\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return grad_x\n",
    "\n",
    "\n",
    "def l2_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient of L2 loss.\n",
    "\n",
    "    loss = 0.5 * sum_i (x_i - y_i)**2 / N\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, D)\n",
    "    - y: Output data, of shape (N, D)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - grad_x: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    diff = x - y\n",
    "    loss = 0.5 * np.sum(diff * diff) / N\n",
    "    grad_x = diff / N\n",
    "    return loss, grad_x\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax (cross-entropy) loss function.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Numpy array of shape (N, C) giving predicted class scores, where\n",
    "      x[i, c] gives the predicted score for class c on input sample i\n",
    "    - y: Numpy array of shape (N,) giving ground-truth labels, where\n",
    "      y[i] = c means that input sample i has ground truth label c, where\n",
    "      0 <= c < C.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - grad_x: Numpy array of shape (N, C) giving the gradient of the loss with\n",
    "      with respect to x\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # Shift the logits by subtracting the maximum value in each row for numerical stability\n",
    "    x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the softmax scores\n",
    "    exp_scores = np.exp(x_shifted)\n",
    "    softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    correct_class_scores = softmax_scores[np.arange(N), y]\n",
    "    loss = -np.sum(np.log(correct_class_scores)) / N\n",
    "\n",
    "    # Compute the gradient of the loss with respect to x\n",
    "    grad_x = softmax_scores.copy()\n",
    "    grad_x[np.arange(N), y] -= 1\n",
    "    grad_x /= N\n",
    "\n",
    "    return loss, grad_x\n",
    "\n",
    "\n",
    "def l2_regularization(w, reg):\n",
    "    \"\"\"\n",
    "    Computes loss and gradient for L2 regularization of a weight matrix:\n",
    "\n",
    "    loss = (reg / 2) * sum_i w_i^2\n",
    "\n",
    "    Where the sum ranges over all elements of w.\n",
    "\n",
    "    Inputs:\n",
    "    - w: Numpy array of any shape\n",
    "    - reg: float giving the regularization strength\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    loss, grad_w = None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement L2 regularization.                                      #\n",
    "    ###########################################################################\n",
    "    loss = (reg / 2) * np.sum(w ** 2)\n",
    "    \n",
    "    grad_w = reg * w\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return loss, grad_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfb538-e6be-4a41-a573-50c6f91acbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Running] python -u \"c:\\Users\\hacknet13\\Desktop\\Î¿ÏÎ±ÏƒÎ·\\assignment2\\gradcheck_layers.py\"\n",
    "Running numeric gradient check for fc\n",
    "  grad_x difference:  5.212257292441791e-10\n",
    "  grad_w difference:  3.5200753423225706e-10\n",
    "  grad_b difference:  1.7937495933040282e-10\n",
    "Running numeric gradient check for relu\n",
    "  grad_x difference:  9.115641574908295e-11\n",
    "Running numeric gradient check for softmax loss\n",
    "  grad_x difference:  1.03187236533131e-10\n",
    "Running numeric gradient check for L2 regularization\n",
    "  grad_w difference:  4.354671657991194e-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458c562-ab8e-443e-bdeb-9f3f6533ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from classifier import Classifier\n",
    "from layers import fc_forward, fc_backward, relu_forward, relu_backward\n",
    "\n",
    "\n",
    "class TwoLayerNet(Classifier):\n",
    "    \"\"\"\n",
    "    A neural network with two layers, using a ReLU nonlinearity on its one\n",
    "    hidden layer. That is, the architecture should be:\n",
    "\n",
    "    input -> FC layer -> ReLU layer -> FC layer -> scores\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=3072, num_classes=10, hidden_dim=512,\n",
    "                 weight_scale=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize a new two layer network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: The number of dimensions in the input.\n",
    "        - num_classes: The number of classes over which to classify\n",
    "        - hidden_dim: The size of the hidden layer\n",
    "        - weight_scale: The weight matrices of the model will be initialized\n",
    "          from a Gaussian distribution with standard deviation equal to\n",
    "          weight_scale. The bias vectors of the model will always be\n",
    "          initialized to zero.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        # TODO: Initialize the weights and biases of a two-layer network.     #\n",
    "        #######################################################################\n",
    "        \n",
    "       \n",
    "        self.W1 = weight_scale * np.random.randn(input_dim, hidden_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = weight_scale * np.random.randn(hidden_dim, num_classes)\n",
    "        self.b2 = np.zeros(num_classes)\n",
    "        self.params = self.parameters()\n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        params = None\n",
    "        #######################################################################\n",
    "        # TODO: Build a dict of all learnable parameters of this model.       #\n",
    "        #######################################################################\n",
    "\n",
    "        params = {\n",
    "            'W1':self.W1,\n",
    "            'b1':self.b1,\n",
    "            'W2':self.W2,\n",
    "            'b2':self.b2,\n",
    "        }\n",
    "        \n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "        return params\n",
    "\n",
    "    def forward(self, X):\n",
    "        scores, cache = None, None\n",
    "        #######################################################################\n",
    "        # TODO: Implement the forward pass to compute classification scores   #\n",
    "        # for the input data X. Store into cache any data that will be needed #\n",
    "        # during the backward pass.                                           #\n",
    "        #######################################################################\n",
    "        hiden_layer , hiden_layer_cache  = fc_forward(X,self.params['W1'],self.params['b1'])\n",
    "        relu_layer,relu_cache = relu_forward(hiden_layer)\n",
    "        scores,fc2_cache = fc_forward(relu_layer,self.params['W2'],self.params['b2'])\n",
    "\n",
    "        # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Ï‰Î½ cache ÏƒÎµ Î­Î½Î± dictionary\n",
    "        cache = {\n",
    "            'hiden_layer_cache': hiden_layer_cache,\n",
    "            'relu_cache': relu_cache,\n",
    "            'fc2_cache': fc2_cache\n",
    "        }\n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "        return scores, cache\n",
    "\n",
    "    def backward(self, grad_scores, cache):\n",
    "        grads = None\n",
    "        #######################################################################\n",
    "        # TODO: Implement the backward pass to compute gradients for all      #\n",
    "        # learnable parameters of the model, storing them in the grads dict   #\n",
    "        # above. The grads dict should give gradients for all parameters in   #\n",
    "        # the dict returned by model.parameters().                            #\n",
    "        #######################################################################\n",
    "       \n",
    "        # input <- FC layer <- ReLU layer <- FC layer <- scores\n",
    "         \n",
    "        hiden_layer_cache = cache['hiden_layer_cache']\n",
    "        relu_cache = cache['relu_cache']\n",
    "        fc2_cache = cache['fc2_cache']\n",
    "       \n",
    "\n",
    "        drelu_layer,grad_W2,grad_b2 = fc_backward(grad_scores,fc2_cache)\n",
    "        dhidden_layer  = relu_backward(drelu_layer,relu_cache)\n",
    "        grad_f2,grad_W1,grad_b1 = fc_backward(dhidden_layer,hiden_layer_cache)\n",
    "\n",
    "        grads = {\n",
    "            'W1': grad_W1,\n",
    "            'b1': grad_b1,\n",
    "            'W2': grad_W2,\n",
    "            'b2': grad_b2,\n",
    "        }\n",
    "\n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5f3e0-b7d7-4bdf-aa3e-630fc31e39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Running] python -u \"c:\\Users\\hacknet13\\Desktop\\Î¿ÏÎ±ÏƒÎ·\\assignment2\\gradcheck_classifier.py\"\n",
    "Running numeric gradient check for LinearClassifier\n",
    "  Max diff for grad_W:  1.734723475976807e-13\n",
    "  Max diff for grad_b:  1.5076828674409626e-13\n",
    "Running numeric gradient check for TwoLayerNet\n",
    "  Max diff for grad_W1:  7.745540320236444e-16\n",
    "  Max diff for grad_b1:  6.336077496005288e-16\n",
    "  Max diff for grad_W2:  5.126107871511465e-16\n",
    "  Max diff for grad_b2:  4.440892098500626e-16\n",
    "\n",
    "[Done] exited with code=0 in 0.264 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772b320-2a9a-4b6b-877d-cdf95c8f4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data import load_cifar10, DataSampler\n",
    "from linear_classifier import LinearClassifier\n",
    "from two_layer_net import TwoLayerNet\n",
    "from optim import SGD\n",
    "from layers import softmax_loss, l2_regularization\n",
    "from utils import check_accuracy\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--plot-file',\n",
    "    default='plot.pdf',\n",
    "    help='File where loss and accuracy plot should be saved')\n",
    "parser.add_argument(\n",
    "    '--checkpoint-file',\n",
    "    default='checkpoint.pkl',\n",
    "    help='File where trained model weights should be saved')\n",
    "parser.add_argument(\n",
    "    '--print-every',\n",
    "    type=int,\n",
    "    default=25,\n",
    "    help='How often to print losses during training')\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # How much data to use for training\n",
    "    num_train = 25000\n",
    "\n",
    "    # Model architecture hyperparameters.\n",
    "    hidden_dim = 128 #64\n",
    "\n",
    "    # Optimization hyperparameters.\n",
    "    batch_size = 64 # 64 \n",
    "    num_epochs = 15 #30\n",
    "    learning_rate = 0.1 #0.01\n",
    "    reg = 0.001 #0.01\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Set hyperparameters for training your model. You can change any   #\n",
    "    # of the hyperparameters above.                                           #\n",
    "    ###########################################################################\n",
    "     # How much data to use for training\n",
    "    # ÎŸÎ»ÎµÏ‚ Î¿Î¹ Ï„Î¹Î¼ÎµÏ‚ Î²ÏÎµÎ¸Î·ÎºÎ±Î½ Ï€ÎµÎ¹ÏÎ±Î¼Î±Ï„Î¹ÎºÎ± /Ï„Ï…Ï‡Î±Î¹Î±  \n",
    "    num_train = 25000\n",
    "\n",
    "    # Model architecture hyperparameters.\n",
    "    hidden_dim = 128 \n",
    "\n",
    "    # Optimization hyperparameters.\n",
    "    batch_size = 64 \n",
    "    num_epochs = 15 \n",
    "    learning_rate = 0.1 \n",
    "    reg = 0.001 \n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "\n",
    "    data = load_cifar10(num_train=num_train)\n",
    "    train_sampler = DataSampler(data['X_train'], data['y_train'], batch_size)\n",
    "    val_sampler = DataSampler(data['X_val'], data['y_val'], batch_size)\n",
    "\n",
    "    # Set up the model and optimizer\n",
    "    model = TwoLayerNet(hidden_dim=hidden_dim)\n",
    "    optimizer = SGD(model.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "    stats = {\n",
    "        't': [],\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f'Starting epoch {epoch} / {num_epochs}')\n",
    "        for i, (X_batch, y_batch) in enumerate(train_sampler):\n",
    "            loss, grads = training_step(model, X_batch, y_batch, reg)\n",
    "            optimizer.step(grads)\n",
    "            if i % args.print_every == 0:\n",
    "                print(f'  Iteration {i} / {len(train_sampler)}, loss = {loss}')\n",
    "            stats['t'].append(i / len(train_sampler) + epoch - 1)\n",
    "            stats['loss'].append(loss)\n",
    "\n",
    "        print('Checking accuracy')\n",
    "        train_acc = check_accuracy(model, train_sampler)\n",
    "        print(f'  Train: {train_acc:.2f}')\n",
    "        val_acc = check_accuracy(model, val_sampler)\n",
    "        print(f'  Val:   {val_acc:.2f}')\n",
    "        stats['train_acc'].append(train_acc)\n",
    "        stats['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Saving plot to {args.plot_file}')\n",
    "    plot_stats(stats, args.plot_file)\n",
    "    print(f'Saving model checkpoint to {args.checkpoint_file}')\n",
    "    model.save(args.checkpoint_file)\n",
    "\n",
    "\n",
    "def training_step(model, X_batch, y_batch, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a single training iteration of a model\n",
    "    given a minibatch of data. The loss should be a sum of a cross-entropy loss\n",
    "    between the model predictions and the ground-truth image labels, and\n",
    "    an L2 regularization term on all weight matrices in the fully-connected\n",
    "    layers of the model. You should not regularize the bias vectors.\n",
    "\n",
    "    Inputs:\n",
    "    - model: A Classifier instance\n",
    "    - X_batch: A numpy array of shape (N, D) giving a minibatch of images\n",
    "    - y_batch: A numpy array of shape (N,) where 0 <= y_batch[i] < C is the\n",
    "      ground-truth label for the image X_batch[i]\n",
    "    - reg: A float giving the strength of L2 regularization to use.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: A float giving the loss (data loss + regularization loss) for the\n",
    "      model on this minibatch of data\n",
    "    - grads: A dictionary giving gradients of the loss with respect to the\n",
    "      parameters of the model. In particular grads[k] should be the gradient\n",
    "      of the loss with respect to model.parameters()[k].\n",
    "    \"\"\"\n",
    "    loss, grads = None, {}\n",
    "    ###########################################################################\n",
    "    # TODO: Compute the loss and gradient for one training iteration.         #\n",
    "    ###########################################################################\n",
    "     \n",
    "     # Forward pass\n",
    "    scores, cache = model.forward(X_batch)\n",
    "    \n",
    "    # data loss Î¼Îµ softmax\n",
    "    loss_softmax,grad_x = softmax_loss(scores,y_batch)\n",
    "\n",
    "    # regularization loss\n",
    "    reg_loss = 0\n",
    "    for param in model.parameters():\n",
    "        if param.startswith('W'):\n",
    "            loss_l2_regularization,grad_w = l2_regularization( model.parameters()[param],reg)\n",
    "            reg_loss += loss_l2_regularization\n",
    "   \n",
    "    # Total loss\n",
    "    loss = loss_softmax + reg_loss\n",
    "    \n",
    "    \n",
    "    # Backward pass\n",
    "    grads = model.backward(grad_x, cache)\n",
    "    \n",
    "    # Add regularization gradient\n",
    "    for param in model.parameters():\n",
    "        if param.startswith('W'):\n",
    "            grads[param] += l2_regularization(model.parameters()[param],reg)[1]\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "def plot_stats(stats, filename):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(stats['t'], stats['loss'], 'o', alpha=0.5, ms=4)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    loss_xlim = plt.xlim()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    epoch = np.arange(1, 1 + len(stats['train_acc']))\n",
    "    plt.plot(epoch, stats['train_acc'], '-o', label='train')\n",
    "    plt.plot(epoch, stats['val_acc'], '-o', label='val')\n",
    "    plt.xlim(loss_xlim)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.gcf().set_size_inches(12, 4)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(parser.parse_args())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5364547-d44c-48f0-8788-6d7a18b493dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Running] python -u \"c:\\Users\\hacknet13\\Desktop\\Î¿ÏÎ±ÏƒÎ·\\assignment2\\train.py\"\n",
    "Starting epoch 1 / 15\n",
    "  Iteration 0 / 390, loss = 2.302812487130162\n",
    "  Iteration 25 / 390, loss = 2.302247769744271\n",
    "  Iteration 50 / 390, loss = 2.2769513000500488\n",
    "  Iteration 75 / 390, loss = 2.1337484288711472\n",
    "  Iteration 100 / 390, loss = 2.152143450545982\n",
    "  Iteration 125 / 390, loss = 2.034679016074625\n",
    "  Iteration 150 / 390, loss = 2.1002640195030917\n",
    "  Iteration 175 / 390, loss = 2.0849698447365896\n",
    "  Iteration 200 / 390, loss = 2.098860917372953\n",
    "  Iteration 225 / 390, loss = 2.0030333154738393\n",
    "  Iteration 250 / 390, loss = 2.080701750827813\n",
    "  Iteration 275 / 390, loss = 2.055137069912502\n",
    "  Iteration 300 / 390, loss = 2.0478431945407576\n",
    "  Iteration 325 / 390, loss = 2.16213865784662\n",
    "  Iteration 350 / 390, loss = 1.8438728764669037\n",
    "  Iteration 375 / 390, loss = 1.9404651979371248\n",
    "Checking accuracy\n",
    "  Train: 29.26\n",
    "  Val:   29.06\n",
    "Starting epoch 2 / 15\n",
    "  Iteration 0 / 390, loss = 1.802015293972788\n",
    "  Iteration 25 / 390, loss = 1.9062247385467046\n",
    "  Iteration 50 / 390, loss = 1.9875072008268875\n",
    "  Iteration 75 / 390, loss = 1.965407012359769\n",
    "  Iteration 100 / 390, loss = 1.8729739746418415\n",
    "  Iteration 125 / 390, loss = 1.9537316797469524\n",
    "  Iteration 150 / 390, loss = 1.8265086920000944\n",
    "  Iteration 175 / 390, loss = 1.8304660164972633\n",
    "  Iteration 200 / 390, loss = 1.9540957803071093\n",
    "  Iteration 225 / 390, loss = 1.8423138563360673\n",
    "  Iteration 250 / 390, loss = 1.7215526622897306\n",
    "  Iteration 275 / 390, loss = 1.8436482889086725\n",
    "  Iteration 300 / 390, loss = 1.6603242287774551\n",
    "  Iteration 325 / 390, loss = 2.0085837537719935\n",
    "  Iteration 350 / 390, loss = 1.807436407651696\n",
    "  Iteration 375 / 390, loss = 1.7790535171477353\n",
    "Checking accuracy\n",
    "  Train: 32.23\n",
    "  Val:   31.82\n",
    "Starting epoch 3 / 15\n",
    "  Iteration 0 / 390, loss = 1.8760393846586774\n",
    "  Iteration 25 / 390, loss = 1.8089585216756998\n",
    "  Iteration 50 / 390, loss = 1.901058349091459\n",
    "  Iteration 75 / 390, loss = 1.8266456046848578\n",
    "  Iteration 100 / 390, loss = 1.7455485520704463\n",
    "  Iteration 125 / 390, loss = 1.8098830472875742\n",
    "  Iteration 150 / 390, loss = 1.6378087970846789\n",
    "  Iteration 175 / 390, loss = 1.6997427591750895\n",
    "  Iteration 200 / 390, loss = 1.6208364886815334\n",
    "  Iteration 225 / 390, loss = 1.7630047082363633\n",
    "  Iteration 250 / 390, loss = 1.9792444083934253\n",
    "  Iteration 275 / 390, loss = 1.6822973888730706\n",
    "  Iteration 300 / 390, loss = 1.878922105140713\n",
    "  Iteration 325 / 390, loss = 1.6891645220846179\n",
    "  Iteration 350 / 390, loss = 1.6686017745088293\n",
    "  Iteration 375 / 390, loss = 2.114635009302929\n",
    "Checking accuracy\n",
    "  Train: 35.33\n",
    "  Val:   34.56\n",
    "Starting epoch 4 / 15\n",
    "  Iteration 0 / 390, loss = 1.6704925379509143\n",
    "  Iteration 25 / 390, loss = 1.5847052709807135\n",
    "  Iteration 50 / 390, loss = 1.9757798832638391\n",
    "  Iteration 75 / 390, loss = 1.8557867905217305\n",
    "  Iteration 100 / 390, loss = 1.8562783436583388\n",
    "  Iteration 125 / 390, loss = 1.610880810842005\n",
    "  Iteration 150 / 390, loss = 1.8130651794849755\n",
    "  Iteration 175 / 390, loss = 1.7523245578081912\n",
    "  Iteration 200 / 390, loss = 1.6779966202733687\n",
    "  Iteration 225 / 390, loss = 1.7528849997759397\n",
    "  Iteration 250 / 390, loss = 1.6118381176725796\n",
    "  Iteration 275 / 390, loss = 1.809316906818264\n",
    "  Iteration 300 / 390, loss = 1.794000295599542\n",
    "  Iteration 325 / 390, loss = 1.859250917812006\n",
    "  Iteration 350 / 390, loss = 1.7258625759305561\n",
    "  Iteration 375 / 390, loss = 1.6871158114100246\n",
    "Checking accuracy\n",
    "  Train: 33.63\n",
    "  Val:   32.66\n",
    "Starting epoch 5 / 15\n",
    "  Iteration 0 / 390, loss = 1.5928608520360699\n",
    "  Iteration 25 / 390, loss = 1.787552911753\n",
    "  Iteration 50 / 390, loss = 1.7573229335895237\n",
    "  Iteration 75 / 390, loss = 1.8490103284593342\n",
    "  Iteration 100 / 390, loss = 1.797686405846781\n",
    "  Iteration 125 / 390, loss = 1.7036989263406934\n",
    "  Iteration 150 / 390, loss = 1.6489186440697705\n",
    "  Iteration 175 / 390, loss = 1.5801980109890839\n",
    "  Iteration 200 / 390, loss = 1.7294566703118919\n",
    "  Iteration 225 / 390, loss = 1.6290947741135584\n",
    "  Iteration 250 / 390, loss = 1.8656518138695086\n",
    "  Iteration 275 / 390, loss = 1.7758916320671068\n",
    "  Iteration 300 / 390, loss = 1.8635795389529337\n",
    "  Iteration 325 / 390, loss = 1.9051931585743622\n",
    "  Iteration 350 / 390, loss = 1.9244196428779632\n",
    "  Iteration 375 / 390, loss = 1.7015692438784589\n",
    "Checking accuracy\n",
    "  Train: 31.93\n",
    "  Val:   31.64\n",
    "Starting epoch 6 / 15\n",
    "  Iteration 0 / 390, loss = 2.018971675070397\n",
    "  Iteration 25 / 390, loss = 1.6288238234650003\n",
    "  Iteration 50 / 390, loss = 1.5987655000103203\n",
    "  Iteration 75 / 390, loss = 1.8642156536008316\n",
    "  Iteration 100 / 390, loss = 1.558825593634985\n",
    "  Iteration 125 / 390, loss = 1.8216544400473058\n",
    "  Iteration 150 / 390, loss = 1.5889963929394977\n",
    "  Iteration 175 / 390, loss = 1.7274310735139724\n",
    "  Iteration 200 / 390, loss = 1.6514306174061157\n",
    "  Iteration 225 / 390, loss = 1.658133446729895\n",
    "  Iteration 250 / 390, loss = 1.831959617186306\n",
    "  Iteration 275 / 390, loss = 1.7354058951557765\n",
    "  Iteration 300 / 390, loss = 1.9110894773991243\n",
    "  Iteration 325 / 390, loss = 1.6938985418342616\n",
    "  Iteration 350 / 390, loss = 1.704590119404481\n",
    "  Iteration 375 / 390, loss = 1.7229911910029323\n",
    "Checking accuracy\n",
    "  Train: 40.50\n",
    "  Val:   38.40\n",
    "Starting epoch 7 / 15\n",
    "  Iteration 0 / 390, loss = 1.5789177113297865\n",
    "  Iteration 25 / 390, loss = 1.7499493484462632\n",
    "  Iteration 50 / 390, loss = 1.639532651514128\n",
    "  Iteration 75 / 390, loss = 1.8339197295746337\n",
    "  Iteration 100 / 390, loss = 1.6308760607029213\n",
    "  Iteration 125 / 390, loss = 1.7167796093312104\n",
    "  Iteration 150 / 390, loss = 1.6465026165481789\n",
    "  Iteration 175 / 390, loss = 2.0868139599089934\n",
    "  Iteration 200 / 390, loss = 1.7371645538504243\n",
    "  Iteration 225 / 390, loss = 1.7074315498395776\n",
    "  Iteration 250 / 390, loss = 1.6692927524353038\n",
    "  Iteration 275 / 390, loss = 1.6566710886558593\n",
    "  Iteration 300 / 390, loss = 1.7627764499923462\n",
    "  Iteration 325 / 390, loss = 1.6040125806343504\n",
    "  Iteration 350 / 390, loss = 2.002236782620586\n",
    "  Iteration 375 / 390, loss = 1.5892360550142361\n",
    "Checking accuracy\n",
    "  Train: 37.50\n",
    "  Val:   36.30\n",
    "Starting epoch 8 / 15\n",
    "  Iteration 0 / 390, loss = 1.9760729420058074\n",
    "  Iteration 25 / 390, loss = 1.9931983405070504\n",
    "  Iteration 50 / 390, loss = 1.631982365791676\n",
    "  Iteration 75 / 390, loss = 1.9430158332765148\n",
    "  Iteration 100 / 390, loss = 1.6461918157372748\n",
    "  Iteration 125 / 390, loss = 1.7710908939936902\n",
    "  Iteration 150 / 390, loss = 1.6029703227507315\n",
    "  Iteration 175 / 390, loss = 1.4929179341675538\n",
    "  Iteration 200 / 390, loss = 1.6977633198939475\n",
    "  Iteration 225 / 390, loss = 1.6388774520081462\n",
    "  Iteration 250 / 390, loss = 1.5631993299885654\n",
    "  Iteration 275 / 390, loss = 1.5877519842992653\n",
    "  Iteration 300 / 390, loss = 1.8620947661241007\n",
    "  Iteration 325 / 390, loss = 1.6282771086896874\n",
    "  Iteration 350 / 390, loss = 1.505497689363071\n",
    "  Iteration 375 / 390, loss = 1.5151486167473414\n",
    "Checking accuracy\n",
    "  Train: 37.73\n",
    "  Val:   35.62\n",
    "Starting epoch 9 / 15\n",
    "  Iteration 0 / 390, loss = 1.6141198434818722\n",
    "  Iteration 25 / 390, loss = 1.3583345726687934\n",
    "  Iteration 50 / 390, loss = 1.719798818779352\n",
    "  Iteration 75 / 390, loss = 1.5732085584837379\n",
    "  Iteration 100 / 390, loss = 1.654123869170234\n",
    "  Iteration 125 / 390, loss = 1.768664064651189\n",
    "  Iteration 150 / 390, loss = 1.6021421086081842\n",
    "  Iteration 175 / 390, loss = 1.6124270489358414\n",
    "  Iteration 200 / 390, loss = 1.9140391702235207\n",
    "  Iteration 225 / 390, loss = 1.630508425596144\n",
    "  Iteration 250 / 390, loss = 1.7330043114713827\n",
    "  Iteration 275 / 390, loss = 1.85326560323153\n",
    "  Iteration 300 / 390, loss = 1.511888984285417\n",
    "  Iteration 325 / 390, loss = 1.7814128705273315\n",
    "  Iteration 350 / 390, loss = 1.7296180494976787\n",
    "  Iteration 375 / 390, loss = 1.4960425883463022\n",
    "Checking accuracy\n",
    "  Train: 42.53\n",
    "  Val:   40.46\n",
    "Starting epoch 10 / 15\n",
    "  Iteration 0 / 390, loss = 1.5760547658296882\n",
    "  Iteration 25 / 390, loss = 1.3634582717979664\n",
    "  Iteration 50 / 390, loss = 1.8124273288999135\n",
    "  Iteration 75 / 390, loss = 1.7634297632024611\n",
    "  Iteration 100 / 390, loss = 1.667300663709414\n",
    "  Iteration 125 / 390, loss = 1.462076859856392\n",
    "  Iteration 150 / 390, loss = 1.845326081063442\n",
    "  Iteration 175 / 390, loss = 1.433952855153633\n",
    "  Iteration 200 / 390, loss = 1.4376373928948059\n",
    "  Iteration 225 / 390, loss = 1.6197659963060462\n",
    "  Iteration 250 / 390, loss = 1.5826671356924047\n",
    "  Iteration 275 / 390, loss = 1.7436626517095375\n",
    "  Iteration 300 / 390, loss = 1.4613603570975855\n",
    "  Iteration 325 / 390, loss = 1.5747363930530214\n",
    "  Iteration 350 / 390, loss = 1.5625465303260189\n",
    "  Iteration 375 / 390, loss = 1.5043938628386915\n",
    "Checking accuracy\n",
    "  Train: 38.39\n",
    "  Val:   37.30\n",
    "Starting epoch 11 / 15\n",
    "  Iteration 0 / 390, loss = 1.7805730983372947\n",
    "  Iteration 25 / 390, loss = 1.4359908638079077\n",
    "  Iteration 50 / 390, loss = 1.4543102368798633\n",
    "  Iteration 75 / 390, loss = 1.4665410788740048\n",
    "  Iteration 100 / 390, loss = 1.6267336538718833\n",
    "  Iteration 125 / 390, loss = 2.013311726212984\n",
    "  Iteration 150 / 390, loss = 1.853382646349985\n",
    "  Iteration 175 / 390, loss = 1.5236054612555725\n",
    "  Iteration 200 / 390, loss = 1.5621146030788526\n",
    "  Iteration 225 / 390, loss = 1.6167487304655013\n",
    "  Iteration 250 / 390, loss = 1.7114455984049195\n",
    "  Iteration 275 / 390, loss = 1.7693567400891286\n",
    "  Iteration 300 / 390, loss = 1.7592221096252971\n",
    "  Iteration 325 / 390, loss = 1.7281106211467412\n",
    "  Iteration 350 / 390, loss = 1.5429931927788334\n",
    "  Iteration 375 / 390, loss = 1.5155853765572913\n",
    "Checking accuracy\n",
    "  Train: 38.58\n",
    "  Val:   35.54\n",
    "Starting epoch 12 / 15\n",
    "  Iteration 0 / 390, loss = 1.672019214776769\n",
    "  Iteration 25 / 390, loss = 1.4178743843016541\n",
    "  Iteration 50 / 390, loss = 1.6083405830806476\n",
    "  Iteration 75 / 390, loss = 1.6651226741047722\n",
    "  Iteration 100 / 390, loss = 1.6417718406286657\n",
    "  Iteration 125 / 390, loss = 1.5844939787797412\n",
    "  Iteration 150 / 390, loss = 1.6517690689480526\n",
    "  Iteration 175 / 390, loss = 1.477691961408688\n",
    "  Iteration 200 / 390, loss = 1.5351247649882553\n",
    "  Iteration 225 / 390, loss = 1.4364514625713412\n",
    "  Iteration 250 / 390, loss = 1.5690248740028903\n",
    "  Iteration 275 / 390, loss = 1.6772870506825914\n",
    "  Iteration 300 / 390, loss = 1.606994074734868\n",
    "  Iteration 325 / 390, loss = 1.5690210568093024\n",
    "  Iteration 350 / 390, loss = 1.8113758611218524\n",
    "  Iteration 375 / 390, loss = 1.6132928442234198\n",
    "Checking accuracy\n",
    "  Train: 45.44\n",
    "  Val:   41.64\n",
    "Starting epoch 13 / 15\n",
    "  Iteration 0 / 390, loss = 1.7672957415404034\n",
    "  Iteration 25 / 390, loss = 1.6845141710153995\n",
    "  Iteration 50 / 390, loss = 1.519767357149524\n",
    "  Iteration 75 / 390, loss = 1.4980187617996552\n",
    "  Iteration 100 / 390, loss = 1.757012945058896\n",
    "  Iteration 125 / 390, loss = 2.007309470515392\n",
    "  Iteration 150 / 390, loss = 1.5536774627378642\n",
    "  Iteration 175 / 390, loss = 1.5982265788658836\n",
    "  Iteration 200 / 390, loss = 1.7925597899488912\n",
    "  Iteration 225 / 390, loss = 1.6416416796220075\n",
    "  Iteration 250 / 390, loss = 1.6843829673603163\n",
    "  Iteration 275 / 390, loss = 1.470577960330473\n",
    "  Iteration 300 / 390, loss = 1.729254596129988\n",
    "  Iteration 325 / 390, loss = 1.7655198677505806\n",
    "  Iteration 350 / 390, loss = 1.5082243520146845\n",
    "  Iteration 375 / 390, loss = 1.6367702604470626\n",
    "Checking accuracy\n",
    "  Train: 45.86\n",
    "  Val:   42.10\n",
    "Starting epoch 14 / 15\n",
    "  Iteration 0 / 390, loss = 1.701548879146913\n",
    "  Iteration 25 / 390, loss = 1.796310704885206\n",
    "  Iteration 50 / 390, loss = 1.721824609441506\n",
    "  Iteration 75 / 390, loss = 1.7004960729059686\n",
    "  Iteration 100 / 390, loss = 1.6307895092768872\n",
    "  Iteration 125 / 390, loss = 1.6045074342857566\n",
    "  Iteration 150 / 390, loss = 1.524937915890564\n",
    "  Iteration 175 / 390, loss = 1.513859465563515\n",
    "  Iteration 200 / 390, loss = 1.4852022882335423\n",
    "  Iteration 225 / 390, loss = 1.7141548264631286\n",
    "  Iteration 250 / 390, loss = 1.6891448636423547\n",
    "  Iteration 275 / 390, loss = 1.3393094175006732\n",
    "  Iteration 300 / 390, loss = 1.5876369133151458\n",
    "  Iteration 325 / 390, loss = 1.6046729707683982\n",
    "  Iteration 350 / 390, loss = 1.6622850774177451\n",
    "  Iteration 375 / 390, loss = 1.7697793687411818\n",
    "Checking accuracy\n",
    "  Train: 44.20\n",
    "  Val:   40.70\n",
    "Starting epoch 15 / 15\n",
    "  Iteration 0 / 390, loss = 1.565263919204722\n",
    "  Iteration 25 / 390, loss = 1.5543777412316548\n",
    "  Iteration 50 / 390, loss = 1.95167582504843\n",
    "  Iteration 75 / 390, loss = 1.6262069919310354\n",
    "  Iteration 100 / 390, loss = 1.3792817677993923\n",
    "  Iteration 125 / 390, loss = 1.623381304965383\n",
    "  Iteration 150 / 390, loss = 1.5356331851296028\n",
    "  Iteration 175 / 390, loss = 1.5133314440506487\n",
    "  Iteration 200 / 390, loss = 1.5076493581919979\n",
    "  Iteration 225 / 390, loss = 1.5232863184031862\n",
    "  Iteration 250 / 390, loss = 1.8096131326107996\n",
    "  Iteration 275 / 390, loss = 1.6415491051862516\n",
    "  Iteration 300 / 390, loss = 2.005980115338229\n",
    "  Iteration 325 / 390, loss = 1.6466109043539137\n",
    "  Iteration 350 / 390, loss = 1.5498422022465042\n",
    "  Iteration 375 / 390, loss = 1.4972999950058532\n",
    "Checking accuracy\n",
    "  Train: 46.26\n",
    "  Val:   42.74\n",
    "Saving plot to plot.pdf\n",
    "Saving model checkpoint to checkpoint.pkl\n",
    "\n",
    "[Done] exited with code=0 in 145.073 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ef9fb-7f2d-4b1a-8dc9-8475a926a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Running] python -u \"c:\\Users\\hacknet13\\Desktop\\Î¿ÏÎ±ÏƒÎ·\\assignment2\\test.py\"\n",
    "Loading model from checkpoint.pkl\n",
    "Test-set accuracy: 42.57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81de728a-7378-4db6-b114-3626eec082e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
